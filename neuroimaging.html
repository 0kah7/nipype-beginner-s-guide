<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">



<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Nipype Beginner&#39;s Guide &mdash; All you need to know to become an expert in Nipype</title>
    
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="top" title="All you need to know to become an expert in Nipype" href="index.html" />
    <link rel="next" title="Nipype and Neuroimaging" href="nipypeAndNeuroimaging.html" />
    <link rel="prev" title="Introduction to Nipype" href="nipype.html" />
    <meta name="keywords" content="nipype, neuroimaging, pipeline, workflow, parallel, python, neuroscience, python, guide, mri, fmri, dti, tutorial, user guide">
    <script type="text/javascript">

      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-24958678-1']);
      _gaq.push(['_setAllowLinker', true]);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();

    </script>

  </head>
  <body>

    <div class="headertext" >
     <a href="index.html">
        Nipype Beginner's Guide</a>
    </div>

    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="nipypeAndNeuroimaging.html" title="Nipype and Neuroimaging"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="nipype.html" title="Introduction to Nipype"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">Home</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/tableofcontent.html">Table of Contents</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/faq.html">FAQ</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/glossary.html">Glossary</a>|</li>
        <li><a href="https://github.com/miykael/nipype-beginner-s-guide/">github</a>|</li>
        <li><a href="http://nipy.org/nipype/">Nipype</a>|</li>
        <li><a href="http://miykael.github.io/nipype-beginner-s-guide/help.html">Help</a>|</li> 
      </ul>
    </div>
  
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/nipype-beginners-guide-html_logo.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Introduction to Neuroimaging</a><ul>
<li><a class="reference internal" href="#acquisition-of-mri-data">Acquisition of MRI Data</a></li>
<li><a class="reference internal" href="#specifics-of-mri-data">Specifics of MRI Data</a></li>
<li><a class="reference internal" href="#modalities-of-mri-data">Modalities of MRI Data</a><ul>
<li><a class="reference internal" href="#smri-structural-mri">sMRI (structural MRI)</a></li>
<li><a class="reference internal" href="#fmri-functional-mri">fMRI (functional MRI)</a></li>
<li><a class="reference internal" href="#dmri-diffusion-mri">dMRI (diffusion MRI)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#analysis-steps">Analysis Steps</a><ul>
<li><a class="reference internal" href="#step-1-preprocessing">Step 1: Preprocessing</a><ul>
<li><a class="reference internal" href="#slice-timing-correction-fmri-only">Slice Timing Correction (fMRI only)</a></li>
<li><a class="reference internal" href="#motion-correction-fmri-only">Motion Correction (fMRI only)</a></li>
<li><a class="reference internal" href="#artifact-detection-fmri-only">Artifact Detection (fMRI only)</a></li>
<li><a class="reference internal" href="#coregistration">Coregistration</a></li>
<li><a class="reference internal" href="#normalization">Normalization</a></li>
<li><a class="reference internal" href="#smoothing">Smoothing</a></li>
<li><a class="reference internal" href="#segmentation-smri-only">Segmentation (sMRI only)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#step-2-model-specification-and-estimation">Step 2: Model Specification and Estimation</a><ul>
<li><a class="reference internal" href="#the-general-linear-model">The General Linear Model</a></li>
<li><a class="reference internal" href="#potential-problems-of-the-glm-approach">Potential problems of the GLM approach</a></li>
<li><a class="reference internal" href="#example-of-a-design-matrix">Example of a Design Matrix</a></li>
<li><a class="reference internal" href="#model-estimation">Model Estimation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#step-3-statistical-inference">Step 3: Statistical Inference</a><ul>
<li><a class="reference internal" href="#contrast-estimation">Contrast Estimation</a></li>
<li><a class="reference internal" href="#thresholding">Thresholding</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="nipype.html"
                        title="previous chapter">Introduction to Nipype</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="nipypeAndNeuroimaging.html"
                        title="next chapter">Nipype and Neuroimaging</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/neuroimaging.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
        </div>
      </div>
    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">

            <!-- Disqus counter -->
            
  <div class="section" id="introduction-to-neuroimaging">
<h1>Introduction to Neuroimaging<a class="headerlink" href="#introduction-to-neuroimaging" title="Permalink to this headline">¶</a></h1>
<p>In this section, I will introduce you to the basics of analyzing neuroimaging data. I will give a brief explanation of how the neuroimaging data is acquired, how the data is prepared for analysis (also called preprocessing). Finally, I&#8217;ll show how you can analyze your data using a model based on your hypothesis.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This part is only a brief introduction to neuroimaging. Further information on this topic can be found under the <a class="reference external" href="http://miykael.github.io/nipype-beginner-s-guide/glossary.html">Glossary</a> and <a class="reference external" href="http://miykael.github.io/nipype-beginner-s-guide/faq.html">FAQ</a> pages of this beginner&#8217;s guide.</p>
</div>
<div class="section" id="acquisition-of-mri-data">
<h2>Acquisition of MRI Data<a class="headerlink" href="#acquisition-of-mri-data" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="_images/brain.gif"><img alt="_images/brain.gif" class="align-left" src="_images/brain.gif" style="width: 160pt;" /></a>
<p>The technology and physics behind an MRI scanner is quite astonishing. But I won&#8217;t go into the details of how it all works. You do need to know some terms, concepts, and parameters that are used to acquire MRI data in a scanning session and use that to construct useable images.</p>
<p>The brain occupies space, so when we collect data on how it fills space, we call that volume data, and all the volume data needed to create the complete, 3D image of the brain, recorded at one single timepoint and as pictured on the left, is called a <strong>volume</strong>.  The data is measured in <strong>voxels</strong>, which are like the pixels used to display images on your screen, only in 3D. Each voxel has a specific dimension, in this case it is 1mm x 1mm x 1mm: a cube, so it is the same dimension from all sides (isotropic). Each voxel contains one value which stands for the average signal measured at the given location.</p>
<p>A standard anatomical volume, with an isotropic voxel resolution of 1mm contains almost 17 million voxels, which are arranged in a <strong>3D matrix</strong> of 256 x 256 x 256 voxels. The following picture shows a slice &#8211; one layer of the big, 3D matrix &#8211; through a brain volume, and the superimposed grid shows the remaining two dimensions of the voxels.</p>
<a class="reference internal image-reference" href="_images/voxel.png"><img alt="_images/voxel.png" class="align-center" src="_images/voxel.png" style="width: 450pt;" /></a>
<p>As the scanner can&#8217;t measure the whole volume at once it has to measure portions of the brain sequentially in time. This is done by measuring one plane of the brain (generally the horizontal one) after the other. Such a plane is also called a <strong>slice</strong>. The <strong>resolution</strong> of the measured volume data, therefore, depends on the in-plane resolution (the size of the squares in the above image), the number of slices and their thickness (how many layers), and any possible gaps between the layers.</p>
<p>The quality of the measured data depends on the resolution and the following parameters:</p>
<ul class="simple">
<li><strong>repetition time (TR)</strong>: time required to scan one volume</li>
<li><strong>acquisition time (TA)</strong>: time required to scan one slice. TA = TR - (TR/number of slices)</li>
<li><strong>field of view (FOV)</strong>: defines the extent of a slice, e.g. 256mm x 256mm</li>
</ul>
</div>
<div class="section" id="specifics-of-mri-data">
<h2>Specifics of MRI Data<a class="headerlink" href="#specifics-of-mri-data" title="Permalink to this headline">¶</a></h2>
<p>MRI scanners output their neuroimaging data in a raw data format with which most analysis packages cannot work.  <strong>DICOM</strong> is a common, standardized, raw medical image format, but the format of your raw data may be something else; e.g., <strong>PAR/REC</strong> format from Philips scanners. Raw data is saved in <a class="reference external" href="http://nifti.nimh.nih.gov/">k-space &lt;https://en.wikipedia.org/wiki/K-space_%28magnetic_resonance_imaging%29&gt;`format, and it needs to be converted into a format that the analysis packages can use.  The most frequent format for newly generated data is called `NIfTI</a>.  If you are working with older datasets, you may encounter data in <strong>Analyze</strong> format.  MRI data formats will have an <strong>image</strong> and a <strong>header</strong> part.  For NifTI format, they are in the same file (.nii-file), whereas in the older Analyze format, they are in separate files (.img and .hdr-file).</p>
<ul class="simple">
<li>The <strong>image</strong> is the actual data and is represented by a 3D matrix that contains a value (e.g. gray value) for each voxel.</li>
<li>The <strong>header</strong> contains information about the data like voxel dimension, voxel extend in each dimension, number of measured time points, a transformation matrix that places the 3D matrix from the <strong>image</strong> part in a 3D coordinate system, etc.</li>
</ul>
</div>
<div class="section" id="modalities-of-mri-data">
<h2>Modalities of MRI Data<a class="headerlink" href="#modalities-of-mri-data" title="Permalink to this headline">¶</a></h2>
<p>There are many different kinds of acquisition techniques. But the most common ones are structural magnetic resonance imaging (<strong>sMRI</strong>), functional magnetic resonance imaging (<strong>fMRI</strong>) and diffusion tensor imaging (<strong>DTI</strong>).</p>
<div class="section" id="smri-structural-mri">
<h3>sMRI (structural MRI)<a class="headerlink" href="#smri-structural-mri" title="Permalink to this headline">¶</a></h3>
<a class="reference internal image-reference" href="_images/GM.gif"><img alt="_images/GM.gif" class="align-left" src="_images/GM.gif" style="width: 270pt;" /></a>
<p>Structural magnetic resonance imaging (<strong>sMRI</strong>) is a technique for measuring the anatomy of the brain. By measuring the amount of water at a given location, sMRI is capable of acquiring a detailed anatomical picture of our brain. This allows as to accurately distinguish between different types of tissue, such as gray and white matter. Structural images are used for multiple purposes, such as corregistration, normalization, segmentation, and surface reconstruction.</p>
<p>As there is no time pressure during acquisition of anatomical images (the anatomy is not supposed to change while the person is in the scanner), a higher resolution can be used for recording anatomical images, with a voxel extent of 0.2 to 1.5mm, depending on the strength of the magnetic field in the scanner, e.g. 1.5T, 3T or 7T. Grey matter structures are seen in dark, and the white matter structures in bright colors.</p>
</div>
<div class="section" id="fmri-functional-mri">
<h3>fMRI (functional MRI)<a class="headerlink" href="#fmri-functional-mri" title="Permalink to this headline">¶</a></h3>
<a class="reference internal image-reference" href="_images/BOLDresponse.png"><img alt="_images/BOLDresponse.png" class="align-right" src="_images/BOLDresponse.png" style="width: 270pt;" /></a>
<p>Functional magnetic resonance imaging (<strong>fMRI</strong>) is a technique for measuring brain activity. It works by detecting the changes in blood oxygenation and blood flow that occur in response to neural activity. Our brain is capable of so many astonishing things. But as nothing comes from nothing, it needs a lot of energy to sustain its functionality and has to increase the energy locally if additional functions are needed. This neuronal activity requires energy in the form of O2 which is carried by the blood. Therefore, increased function results in increased blood flow towards the energy consuming location.</p>
<p>Immediately after neural activity the blood oxygen level decreases, known as the <em>initial dip</em>, because of the local energy consumption. This is followed by the increased flow of new and oxygen rich blood towards the energy consuming region. After 4-6 seconds a peak of blood oxygen level is reached. After no further neuronal activation takes place the signal decreases again and goes through an undershoot, before it reaches the baseline again.</p>
<p>This blood oxygen level is exactly what we measure with fMRI. The MRI-Scanner is able to measure the change in the magnetic field caused by the difference in the  magnetic susceptibility of oxygenated (diamagnetic) and deoxygenated (paramagnetic) blood. The signal is therefore called the <strong>Blood Oxygen Level Dependent (BOLD) respond</strong>.</p>
<a class="reference internal image-reference" href="_images/WM.gif"><img alt="_images/WM.gif" class="align-left" src="_images/WM.gif" style="width: 270pt;" /></a>
<p>Because the BOLD signal has to be measured very fast, the resolution of functional images is normally lower (2-4mm) than the resolution in a structural images (0.5-1.5mm). But this depends strongly on the strength of the magnetic field in the scanner, e.g. 1.5T, 3T or 7T. In a functional image, the gray matter is seen as bright and the white matter as dark colors, which is the exact opposite to structural images.</p>
<p>Depending on the paradigmn, we talk from an <strong>event-related</strong>, <strong>block</strong> or <strong>resting-state design</strong>:</p>
<ul class="simple">
<li><strong>event-related design</strong>: Event-releated means that the stimuli shown to the participants in the scanner, are only shown briefly and generally in random order. This means that the BOLD response consists of short bursts (peak) and look more or less like the line shown in the picture above.</li>
<li><strong>block design</strong>: If multiple stimulation of similar nature are shown in a block or phase of 10-30 seconds, we talk about a block design. Such a design has the advantages that the peak in the BOLD signal is not just reach for a short period but stays on a plateau for a longer time. This makes it easier to detect an underlying activation increase in the brain.</li>
<li><strong>resting-state design</strong>: Resting-state paradigms are acquisition in the absence of stimulation. Subjects are asked to lay still and rest in the scanner, without falling asleep. The goal of such a scan is to record brain activation in the absence of an external task. This is sometimes done to analyze the functional connectivity of the brain.</li>
</ul>
</div>
<div class="section" id="dmri-diffusion-mri">
<h3>dMRI (diffusion MRI)<a class="headerlink" href="#dmri-diffusion-mri" title="Permalink to this headline">¶</a></h3>
<img alt="_images/tractography_small.gif" class="align-left" src="_images/tractography_small.gif" />
<p>Diffusion imaging is done to get information about the brain&#8217;s white matter connections. There are multiple modalities to record diffusion images, such as diffusion tensor imaging (DTI), diffusion spectrum imaging (DSI), diffusion weighted imaging (DWI) and diffusion functional MRI (DfMRI). By recording the diffusion trajectory of the molecules (usually water) in a given voxel, one can make assumptions about the underlying structure in the voxel. For example, if one voxel contains mostly horizontal fiber tracts, the water molecule in this region will mostly diffuse (move) in a horizontal manner, as they can&#8217;t move vertically because of this neural barrier. The diffusion itself is caused mostly by the <a class="reference external" href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a>.</p>
<p>There are many different diffusion measurements, such as <strong>mean diffusivity</strong> (MD), <a class="reference external" href="https://en.wikipedia.org/wiki/Fractional_anisotropy">fractional anisotropy</a> (FA) and <a class="reference external" href="https://en.wikipedia.org/wiki/Tractography">Tractography</a>. Each measurement gives different insights into the brain&#8217;s neural fiber tracts. An example of a reconstructed tractography can be seen in the image to the left.</p>
<p>Diffusion MRI is a rather new field in MRI and still has some problems with its sensitivity to correctly detect fiber tracts and their underlying orientation. For example, the standard DTI method has almost no chance to reliably detect kissing (touching) or crossing fiber tracts. To account for this disadvantage, newer methods such as <strong>High-angular-resolution diffusion imaging</strong> (HARDI) and Q-ball vector analysis were developed. For more about diffusion MRI see the <a class="reference external" href="https://en.wikipedia.org/wiki/Diffusion_MRI">Diffusion MRI Wiki-homepage</a>.</p>
</div>
</div>
<div class="section" id="analysis-steps">
<h2>Analysis Steps<a class="headerlink" href="#analysis-steps" title="Permalink to this headline">¶</a></h2>
<p>There are many different steps involved in a neuroimaging analysis and there is not just one way to do it. Depending on the researcher, the paradigm at hand or the modality analyzed (sMRI, fMRI, dMRI), the order differs. Some steps might occur earlier or later than usual and other are left out entirely. None the less, the general fMRI analysis can be divided into the following three steps:</p>
<ol class="arabic simple">
<li><strong>Preprocessing</strong>: Spatial and temporal pre processing of data with the intend of preparing it for the 1st and 2nd level analysis.</li>
<li><strong>Model Specification and Estimation</strong>: Specifying and estimating parameters of statistical model</li>
<li><strong>Statistical Inference</strong>: Making inferences about the estimated parameters with appropriate statistics</li>
</ol>
<div class="section" id="step-1-preprocessing">
<h3>Step 1: Preprocessing<a class="headerlink" href="#step-1-preprocessing" title="Permalink to this headline">¶</a></h3>
<p>With the preprocessing we correct our data for head movement in the scanner, check our data for artifacts, take into account that a volume is measured slice by slice, increase the signal-to-noise ratio by smoothing it and normalize it into a common reference space. All those steps are done to match all scans of an individual subject to itself and than to match this subject into a common standard space. Therefor, the preprocessing is done to improve our data and to prepare it for the statistical analysis.</p>
<div class="section" id="slice-timing-correction-fmri-only">
<h4>Slice Timing Correction (fMRI only)<a class="headerlink" href="#slice-timing-correction-fmri-only" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/slicetiming_small.gif"><img alt="_images/slicetiming_small.gif" class="align-right" src="_images/slicetiming_small.gif" style="width: 499px;" /></a>
<p>Because most functional MRI measuring sequences don&#8217;t acquire every slice in a volume at the same time we have to account for that. For example, if you acquire a volume with 37 slices in a ascending fashion and each slice would be acquired every 50ms, there still would be a difference of 1.8s between the first and the last slice. (Left: <em>ascending</em>, Right: <em>interleaved</em>)</p>
<p>Slice Timing Correction is used to control for this time differences between the slice by temporally interpolating the slices so that it would be equivalent to acquiring the whole brain image at a single time point. This temporal factor of acquisition especially has to be accounted for in fMRI models where timing is an important factor (e.g. event related designs, where the type of stimulus can change from volume to volume).</p>
<p>If you use Slice Timing it is also important to know the way the slices were acquired. You can acquire the slices in a descending (top-down), ascending (bottom-up) or interleaved (acquire every second slice on one direction and every other slice on the other direction; interleaved can start in a top-down or bottom-up fashion) way.</p>
</div>
<div class="section" id="motion-correction-fmri-only">
<h4>Motion Correction (fMRI only)<a class="headerlink" href="#motion-correction-fmri-only" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/movement.gif"><img alt="_images/movement.gif" class="align-right" src="_images/movement.gif" style="width: 200pt;" /></a>
<p>Motion correction, also known as Realignment, is used to correct for head movement during the acquisition of functional data. Even small head movements lead to unwanted variance in voxels and minimize the quality of your data. Motion correction tries to minimize the influence of movement on your data by aligning your data to a reference time volume. This reference time volume is usually the mean image of all timepoints but can also be the first time point or which ever one you want to use.</p>
<p>The head movement can be divided into 6 parameters. 3 translation parameters which code the movements in the direction of the 3 dimensional axes (moving in X, Y, or Z direction) and 3 rotation parameters which code the rotation around those axes (rotation over the X, Y and Z axis).</p>
<p>Realignment usually uses a affine rigid body transformation to manipulate the data in those 6 parameters. Below you see a plot of a &#8220;good&#8221; subject where the movement is minimal.</p>
<a class="reference internal image-reference" href="_images/realignment_good.png"><img alt="_images/realignment_good.png" class="align-center" src="_images/realignment_good.png" style="width: 400pt;" /></a>
</div>
<div class="section" id="artifact-detection-fmri-only">
<h4>Artifact Detection (fMRI only)<a class="headerlink" href="#artifact-detection-fmri-only" title="Permalink to this headline">¶</a></h4>
<p>Not all subjects lie perfectly still and as we can see in the example below, some move quite a bit. This sudden movement can be very severe and really can contaminate your analysis.</p>
<a class="reference internal image-reference" href="_images/realignment_bad.png"><img alt="_images/realignment_bad.png" class="align-center" src="_images/realignment_bad.png" style="width: 400pt;" /></a>
<p>The process of motion correction tries to correct for this movement but sometimes it&#8217;s best to just take the scans with extreme rapid movement out. To do this, we use <strong>Artifact Detection</strong>. Artifact detection is used to declare the timepoints/scans of the functional image which vary so much in head movement that they should be excluded from further analysis.</p>
<p>So if I would check the session shown above for sudden movement greater than 2 standard deviation from the mean or for movement greater than 1mm, Artifact Detection would me show that the scans 16-19, 21, 22 and 169-172 (see image below) should be excluded from further analysis.</p>
<img alt="_images/artifact_detection.png" class="align-center" src="_images/artifact_detection.png" />
</div>
<div class="section" id="coregistration">
<h4>Coregistration<a class="headerlink" href="#coregistration" title="Permalink to this headline">¶</a></h4>
<p>As motion correction corrects for the motion during the acquisition of functional images, coregistration corrects for motion between the structural and the functional images. In other words, coregistration moves the functional image as such that it lies perfectly on the anatomical image. This allows further transformations on the anatomical image, such as normalization, to be directly applied to the functional image without any intermediate steps.</p>
<p>The following picture shows an example of a good (top) and bad (bottom) coregistration between functional and anatomical images. Shown in red are the outline of the cortical folds of the anatomical image and in white and gray the functional image.</p>
<a class="reference internal image-reference" href="_images/coregistration.png"><img alt="_images/coregistration.png" class="align-center" src="_images/coregistration.png" style="width: 400pt;" /></a>
</div>
<div class="section" id="normalization">
<h4>Normalization<a class="headerlink" href="#normalization" title="Permalink to this headline">¶</a></h4>
<p>Normalization is done to warp the data of a subject from the individual subject-space it was measured in into a standard reference-space. This step is done to control for individual morphological variations of the brain in each subject. Only after this step a group analysis or comparison to other data can be done. There are different ways to normalize your data but it always includes a template and a source image.</p>
<a class="reference internal image-reference" href="_images/normalization.png"><img alt="_images/normalization.png" class="align-center" src="_images/normalization.png" style="width: 600pt;" /></a>
<ul class="simple">
<li>The <strong>template</strong> image is the standard brain in reference-space that you want to warp your data into. This can be a Talairach-, MNI-, SPM-template or any other reference brain you want to use.</li>
<li>The <strong>source</strong> image (normally a structural image) is used to calculate the transformation matrix necessary to warp the source image onto the template image. This transformation matrix is than used to transform the rest of your images (functional and structural) into the reference-space.</li>
</ul>
</div>
<div class="section" id="smoothing">
<h4>Smoothing<a class="headerlink" href="#smoothing" title="Permalink to this headline">¶</a></h4>
<p>Structural as well as functional images are smoothed by applying a filter to the image. Smoothing increases the signal to noise ratio of your data. This step helps to reduce spatial differences between subject and therefore improve comparisons across subjects. The trade-off, however, is that you lose resolution by smoothing. Additionally you have to keep in mind that smoothing can cause regions that are functionally different to  combine with each other. In such cases a surface based analysis with smoothing on the surface might be a better choice.</p>
<a class="reference internal image-reference" href="_images/smoothed.png"><img alt="_images/smoothed.png" class="align-center" src="_images/smoothed.png" style="width: 500pt;" /></a>
<a class="reference internal image-reference" href="_images/kernel.png"><img alt="_images/kernel.png" class="align-right" src="_images/kernel.png" style="width: 200pt;" /></a>
<p>Smoothing is implemented by applying a 3D Gaussian kernel to the image, defined by its full width at half maximum (<strong>FWHM</strong>) parameter. As the name already says, FWHM specifies the width/diameter of the smoothing kernel on half of it&#8217;s height. Each voxel becomes the result of applying this smoothing kernel as a weighted region of interest to its position.</p>
<p>Choosing the size of the smoothing kernel also depends on the region you are interested in. If you want to study a very small region a big large kernel could eventually smooth your data too much. Thus, the amount of smoothing that you should use is determined partly by the question you want to answer.</p>
</div>
<div class="section" id="segmentation-smri-only">
<h4>Segmentation (sMRI only)<a class="headerlink" href="#segmentation-smri-only" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/segmentation.gif"><img alt="_images/segmentation.gif" class="align-right" src="_images/segmentation.gif" style="width: 200pt;" /></a>
<p>Segmentation stands for the process in which a brain is divided into neurological sections according to a given template segmentation. This can be rather general, by segmenting the brain into gray matter, white matter and cerebrospinal fluid (like it is done with SPM&#8217;s Segmentation) or quite detailed into specific regions and their subregions like it is done during FreeSurfer&#8217;s <code class="docutils literal"><span class="pre">recon-all</span></code> process. This is is also the segmentation you see in this picture.</p>
<p>The Segmentation can be used for different things. You can use the segmentation to aid the normalization process or use it to aid further analysis by using a specific segmentation as a mask or as a definition of a specific region of interest (ROI).</p>
</div>
</div>
<div class="section" id="step-2-model-specification-and-estimation">
<h3>Step 2: Model Specification and Estimation<a class="headerlink" href="#step-2-model-specification-and-estimation" title="Permalink to this headline">¶</a></h3>
<p>To test our hypothesis on our data we first need to specify a model that incorporates this hypothesis and accounts for multiple factors such as the expected function of the BOLD signal, the movement during measurement, experiment specify parameters and other regressors and covariates. Such a model is usually represented by a Generalized Linear Model (GLM).</p>
<div class="section" id="the-general-linear-model">
<h4>The General Linear Model<a class="headerlink" href="#the-general-linear-model" title="Permalink to this headline">¶</a></h4>
<p>A GLM describes a response (y), such as the BOLD response in a voxel, in terms of all its contributing factors (xβ) in a linear combination, whilst also accounting for the contribution of error (ε). The column (y) corresponds to one voxel and one row in this column corresponds to one time-point.</p>
<a class="reference internal image-reference" href="_images/GLM.png"><img alt="_images/GLM.png" class="align-center" src="_images/GLM.png" style="width: 300pt;" /></a>
<ul>
<li><dl class="first docutils">
<dt><strong>y = dependent variable</strong></dt>
<dd><p class="first last">observed data (e.g. BOLD response in a single voxel)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>X = Independent Variable</strong> (aka. Predictor)</dt>
<dd><p class="first last">e.g. <em>experimental conditions</em> (embodies all available knowledge about experimentally controlled factors and potential confounds), <em>stimulus information</em> (onset and duration of stimuli), <em>expected shape of BOLD response</em></p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>β = Parameters</strong> (aka regression coefficient/beta weights)</dt>
<dd><p class="first last">Quantifies how much each predictor (<em>X</em>) independently influences the dependent variable (<em>Y</em>)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>ε = Error</strong></dt>
<dd><p class="first last">Variance in the data (<em>Y</em>) which is not explained by the linear combination of predictors (<em>Xβ</em>). The error is assumed to be normally distributed.</p>
</dd>
</dl>
</li>
</ul>
<p>The predictor variables are stored in a so called <strong>Design Matrix</strong>. The <strong>β</strong> parameters define the contribution of each component of this design matrix to the model. They are estimated so as to minimize the error, and are used to generate the <strong>contrasts</strong> between conditions. The <strong>Errors</strong> is the difference between the observed data and the model defined by Xβ.</p>
</div>
<div class="section" id="potential-problems-of-the-glm-approach">
<h4>Potential problems of the GLM approach<a class="headerlink" href="#potential-problems-of-the-glm-approach" title="Permalink to this headline">¶</a></h4>
<p><strong>BOLD responses have a delayed and dispersed form</strong></p>
<ul class="simple">
<li>We have to take the time delay and the HRF shape of the BOLD response into account when we create our design matrix.</li>
</ul>
<p><strong>BOLD signals include substantial amounts of low-frequency noise</strong></p>
<ul class="simple">
<li>By high pass filtering our data and adding time regressors of 1st, 2nd,... order we can correct for low-frequency drifts in our measured data. This low frequency signals are caused by non-experimental effects, such as scanner drift etc.</li>
</ul>
<a class="reference internal image-reference" href="_images/time.png"><img alt="_images/time.png" class="align-center" src="_images/time.png" style="width: 350pt;" /></a>
<p>This <strong>High pass Filter</strong> is established by setting up discrete cosine functions over the time period of your acquisition. In the example below you see a constant term of 1, followed by half of a cosine function increasing by half a period for each following curve. Such regressors correct for the influence of changes in the low-frequency spectrum.</p>
<a class="reference internal image-reference" href="_images/highpassfilter.png"><img alt="_images/highpassfilter.png" class="align-center" src="_images/highpassfilter.png" style="width: 250pt;" /></a>
</div>
<div class="section" id="example-of-a-design-matrix">
<h4>Example of a Design Matrix<a class="headerlink" href="#example-of-a-design-matrix" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/stimuli.png"><img alt="_images/stimuli.png" class="align-right" src="_images/stimuli.png" style="width: 200pt;" /></a>
<p>Let us assume we have an experiment where we present subjects faces of humans and animals alike. Our goal is to measure the difference between the brain activation when a face of an animal is presented in contrast to the activation of the brain when a human face is presented. Our experiment is set up in such a way that subjects have two different blocks of stimuli presentation. In both blocks there are timepoints where faces of humans, faces of animals and no faces (resting state) are presented.</p>
<p>Now, we combine all that we know about our model into one single Design Matrix. This Matrix contains multiple columns, which contain information about the stimuli (onset, duration and curve function of the BOLD-signal i.e. the shape of the HRF). In our example column <em>Sn(1) humans</em> and <em>Sn(1) animals</em> code for the stimuli of humans and animals during the first session of our fictive experiment. Accordingly, Sn(2) codes for all the regressors in the second session. <em>Sn(1 resting</em> codes for the timepoints where subjects weren&#8217;t presented any stimuli.</p>
<a class="reference internal image-reference" href="_images/designmatrix.png"><img alt="_images/designmatrix.png" class="align-center" src="_images/designmatrix.png" style="width: 350pt;" /></a>
<p>The y-axis codes for the measured scan or the passed time, depending on the specification of your design. The x-axis stands for all the regressors that we specified.</p>
<p>The regressors <em>Sn(1) R1</em> to <em>Sn(1) R6</em> stand for the movement parameters we got from the realignment process. The regressors <em>Sn(1) linear</em>, <em>Sn(1) quadratic</em>, <em>Sn(1) cubic</em> and <em>Sn(1) quartic</em> are just examples of correction for the low frequency in your data. If you are using a high-pass filter of e.g. 128 seconds you don&#8217;t need to specifically include those regressors in your design matrix.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Adding one more regressors to your model decrease the degrees of freedom in your statistical tests by one.</p>
</div>
</div>
<div class="section" id="model-estimation">
<h4>Model Estimation<a class="headerlink" href="#model-estimation" title="Permalink to this headline">¶</a></h4>
<p>After we specified the parameters of our model in a design matrix we are ready to estimate our model. This means that we apply our model on the time course of each and every voxel.</p>
<p>Depending on the software you are using you might get different types of results. If you are using <strong>SPM</strong> the following images are created each time an analysis is performed (1st or 2nd level):</p>
<ul>
<li><dl class="first docutils">
<dt><strong>beta images</strong></dt>
<dd><p class="first last">images of estimated regression coefficients (parameter estimate). beta images contain information about the size of the effect of interest. A given voxel in each beta image will have a value related to the size of effect for that explanatory variable.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>error image</strong> - <code class="docutils literal"><span class="pre">ResMS</span></code>-image</dt>
<dd><p class="first last">residual sum of squares or variance image. It is a measure of within-subject error at the 1st level or between-subject error at the 2nd level analysis. This image is used to produce spmT images.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>con images</strong> - <code class="docutils literal"><span class="pre">con</span></code>-images</dt>
<dd><p class="first last">during contrast estimation beta images are linearly combined to produce relevant <code class="docutils literal"><span class="pre">con</span></code>-images</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>T images</strong> - <code class="docutils literal"><span class="pre">spmT</span></code>-images</dt>
<dd><p class="first last">during contrast estimation the beta values of a <code class="docutils literal"><span class="pre">con</span></code>-image are combined with error values of the <code class="docutils literal"><span class="pre">ResMS</span></code>-image to calculate the t-value at each voxel</p>
</dd>
</dl>
</li>
</ul>
</div>
</div>
<div class="section" id="step-3-statistical-inference">
<h3>Step 3: Statistical Inference<a class="headerlink" href="#step-3-statistical-inference" title="Permalink to this headline">¶</a></h3>
<p>Before we go into the specifics of a statistical analysis, let me explain you the difference between a 1st and a 2nd level analysis.</p>
<dl class="docutils">
<dt><strong>1st level analysis (within-subject)</strong></dt>
<dd>A 1st level analysis is the statistical analysis done on each and every subject by itself. For this procedure the data doesn&#8217;t have to be normalized, i.e in a common reference space. A design matrix on this level controls for subject specific parameters as movement, respiration, heart beat, etc.</dd>
<dt><strong>2nd level analysis (between-subject)</strong></dt>
<dd>A 2nd level analysis is the statistical analysis done on the group. To be able to do this, our subject specific data has to be normalized and transformed from subject-space into reference-space. Otherwise we wouldn&#8217;t be able to compare subjects between each other. Additionally, all contrasts of the 1st level analysis have to be estimated because the model of the 2nd level analysis is conducted on them. The design matrix of the 2nd level analysis controls for subject specific parameters such as age, gender, socio-economic parameters, etc. At this point we also specify the group assignment of each subject.</dd>
</dl>
<div class="section" id="contrast-estimation">
<h4>Contrast Estimation<a class="headerlink" href="#contrast-estimation" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/contrasts.png"><img alt="_images/contrasts.png" class="align-right" src="_images/contrasts.png" style="width: 220pt;" /></a>
<p>Independent of the level of your analysis, after you&#8217;ve specified and estimated your model you now have to estimate the contrasts you are interested in. In such a <strong>contrast</strong> you specify how to weight the different regressors of your design matrix and combine them in one single image.</p>
<p>For example, if you want to compare the brain activation during the presentation of human faces compared to the brain activation during the presentation of animal faces over two sessions you have to weight the regressors <em>Sn(1) humans</em> and <em>Sn(2) humans</em> with 1 and <em>Sn(1) animals</em> and <em>Sn(2) animals</em> with -1, as can be seen in <strong>contrast 3</strong>. This will subtract the value of the animal-activation from the activation during the presentation of human faces. The result is an image where the positive activation stands for &#8220;more active&#8221; during the presentation of human faces than during the presentation of animal faces.</p>
<p>Contrast 1 codes for <em>human faces vs. resting</em>, contrast 2 codes for <em>animal faces vs. resting</em>, contrast 4 codes for <em>animal faces vs. human faces</em> (which is just the inverse image of contrast 3) and contrast 5 codes for <em>session 1 vs. session 2</em>, which looks for regions which were more active in the first session than in the second session.</p>
</div>
<div class="section" id="thresholding">
<h4>Thresholding<a class="headerlink" href="#thresholding" title="Permalink to this headline">¶</a></h4>
<p>After the contrasts are estimated there is only one final step to be taken before you get a scientific based answer to your question. You have to threshold your results. With that I mean, you have to specify the level of significance you want to test your data on, you have to correct for multiple comparison and you have to specify the parameters of the results you are looking for. E.g.:</p>
<ul class="simple">
<li><strong>FWE-correction</strong>: The family-wise error correction is one way to correct for multiple comparisons</li>
<li><strong>p-value</strong>: specify the hight of the significance threshold that you want to use (e.g. z=1.6449 equals p&lt;0.05 (one-tailed); see image)</li>
<li><strong>voxel extend</strong>: specify the minimum size of a &#8220;significant&#8221; cluster by specifying the number of voxel it at least has to contain.</li>
</ul>
<a class="reference internal image-reference" href="_images/pvalues.png"><img alt="_images/pvalues.png" class="align-center" src="_images/pvalues.png" style="width: 350pt;" /></a>
<p>If you do all this correctly, you&#8217;ll end up with something as shown in the following picture. The picture shows you the average brain activation of 20 subjects during the presentation of an acoustic stimuli. The p-value are shown from red to yellow, representing values from 0.05 to 0.00. Shown are only cluster with a voxel extend of at least 100 voxels.</p>
<a class="reference internal image-reference" href="_images/contrast_acoustic.png"><img alt="_images/contrast_acoustic.png" class="align-center" src="_images/contrast_acoustic.png" style="width: 350pt;" /></a>
</div>
</div>
</div>
</div>

   


          </div>
        </div>
      </div>

      <!-- Disqus message -->
      <div id="disqus_thread"></div>
      <script type="text/javascript">
          /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
          var disqus_shortname = 'nipypebeginnersguide'; // required: replace example with your forum shortname
          /* * * DON'T EDIT BELOW THIS LINE * * */
          (function() {
              var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
              dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
              (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
              })();
      </script>
      <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="nipypeAndNeuroimaging.html" title="Nipype and Neuroimaging"
             >next</a></li>
        <li class="right" >
          <a href="nipype.html" title="Introduction to Nipype"
             >previous</a> |</li>
        <li><a href="index.html">Home</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/tableofcontent.html">Table of Contents</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/faq.html">FAQ</a>|</li>
        <li><a href="http://miykael.github.com/nipype-beginner-s-guide/glossary.html">Glossary</a>|</li>
        <li><a href="https://github.com/miykael/nipype-beginner-s-guide/">github</a>|</li>
        <li><a href="http://nipy.org/nipype/">Nipype</a>|</li>
        <li><a href="http://miykael.github.io/nipype-beginner-s-guide/help.html">Help</a>|</li> 
      </ul>
    </div>
    <div class="footer">
    <p>
        &copy; Copyright 2016, Michael Notter.
      Last updated on March 09, 2016.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.3.5.
    </p>
    <p>
    This page uses <a href="http://analytics.google.com/">Google Analytics</a> to collect statistics. You can disable it by blocking
the JavaScript coming from www.google-analytics.com.
    </p>
    </div>
  </body>
</html>